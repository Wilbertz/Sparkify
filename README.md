# Sparkify

## Table of Contents

1. [Installation](#installation)
2. [Directory Structure](#directoryStructure)
3. [Project Motivation](#motivation)
4. [Data Exploration](#exploration)
5. [Features](#features)
6. [Modelling](#modelling)
7. [Results](#results)

## Installation <a name="installation"></a>
This project was written in Python 3.6, using a Jupyter Notebook on Anaconda. The relevant Python packages for this project are as follows:

- numpy
- pandas
- datetime
- pyspark.sql
- pyspark.sql.functions
- pyspark.sql.types
- pyspark.sql.window
- pyspark.ml.feature 
- pyspark.ml.classification
- pyspark.ml.evaluation
- pyspark.ml.tuning
- sklearn.metrics

## Directory Structure <a name="directoryStructure"></a>

- Root /

    - README.md  
    - Sparkify.ipynb
    
## Project Motivation <a name="motivation"></a>
Sparkify is a music streaming service like Spotify. There are paid premium services and free basic services. 
The free users will listen to ads while the paid users consume songs ad-free. At any time users can upgrade, 
downgrade or cancel their subscription. 
We want to identify the users that could potentially cancel their account and leave the service. 
By identifying this population upfront, we could incentive them by giving them discounts or other rewards. 
This could make them stick, giving us a loyal customer base which is important for the company's success.

Within this project churn is defined as a cancellation of service. 
In case a "Cancellation Confirmation" event for a user is detected, this user is considered to have churned. 
There might be multiple cancellation confirmations, due to the fact, 
that a user registered after cancellations but cancelled again.

## Data Exploration <a name="exploration"></a>
All users (free and paid) have to log in in order to use the service. Log data is generated by the user activities at any given time. From these activities we can identify and analyze specific actions (e.g. number of songs played, number thumbs up, etc.).The full data set is 12 GB, but only a subset of 128 Mb will be used. 

The data set has 18 different columns and there are 286,500 records.

There are 8346 rows where the userId is empty. Since there are also 8346 rows where first name and last name is missing (probably the same rows) there exists no substitute for an user id. Therefore these rows have been removed. After removing these rows there are still 50046 (17 %) rows, where essential information like artist or song is missing. Probably these are page hits that don't involve a song like landing page hits.

The data set contains data for 63 days, around 2 months (1 October 2018 until 3 December 2018)
A total of 225 users have interacted with Sparkify in a total of 2312 sessions. The data set contains 58481 different songs from 17656 artists.

49 users downgraded and 52 users cancelled their subscription.
In case of the 52 users who cancelled, 9 downgraded before cancelling, 43 cancelled without a downgrade. In case of the 52 users who cancelled, 31 are paid users, 21 are free users.

## Features <a name="features"></a>

Based upon the observations made during the previous phase of data exploration, a number features was created:
- Session Count:
  The number of distinct sessions per user.
- Songs per Session:
  The average number of songs consumed per session.
- Number of different artists:
  The number of different artist streamed.
- Days since registration:
  The amount of days since registration.
- Average duration of session:
  The average duration of a session.
- Average number of songs per day:
  The average number of songs streamed per day.
- Level (free or paid):
  Whether the user subscribed to the free or paid service level. One-Hot encoding was used to encode this feature.
- Average number of thumbs up per day:
  The average thumbs up clicks per day.
- Average number of thumbs down per day:
  The average thumbs down clicks per day.
- Average number of error pages per day:
  The average number of error pages shown to the user per day.

A dedicated Python method was created, that did all the feature extraction. All features where converted to numeric values and scaled. As a final step, a data frame with 2 columns, a label column with the label column and a single feature column with a list of scaled numeric features was created.

## Modelling <a name="modelling"></a>
The full data-set was split into a training and a validation set. Due to the small number of users that did churn, the split was 60 % for the training set and 40 % for the validation set.
Several machine learning algorithms are available via PySpark, the following algorithms were selected:

- Random Forest Classifier
- Logistic Regression
- Support Vector Machine
- Gradient Boosted Trees

For all algorithms a grid search for optimal hyper-parameters was done.

The F1 score was used to evaluate the results. The F1 score gives equal weight to precision and recall since it is the harmonic mean of both.
The business will spend money on users predicted to churn. In order to avoid spending money on users, that will not churn, a high level of precision is necessary.
But the business would lose customers in case the algorithm would be unable to identify customers that are likely to churn.Therefore a high recall value is desirable.

The following classification results could be achieved:

| Algorithm            | F1 Score  | Precision | Recall | Accuracy |
| -------------------- | --------- | --------- | ------ | -------- |
| Decision Tree        | 0.82      | 0.78      | 0.87   | 0.95     |
| Logistic Regression  | 0.30      | 0.75      | 0.19   | 0.87     | 
| GBT                  | 0.86      | 0.79      | 0.94   | 0.95     |

## Results <a name="results"></a>
The GBT (Gradient Boosted Trees) classifier produced the best results. An F1 score of 85.7 % could be reached after a Grid search. The values for Precision and Recall were 78.9 % and 93.7 %.
